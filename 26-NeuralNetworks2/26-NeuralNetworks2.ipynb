{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 25: Neural Networks II\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll continue discussing Neural Networks. \n",
    "\n",
    "Recommended Reading:\n",
    "* A. Géron, [Hands-On Machine Learning with Scikit-Learn & TensorFlow](http://proquest.safaribooksonline.com/book/programming/9781491962282) (2017) \n",
    "* I. Goodfellow, Y. Bengio, and A. Courville, [Deep Learning](http://www.deeplearningbook.org/) (2016)\n",
    "*  Y. LeCun, Y. Bengio, and G. Hinton, [Deep learning](https://www.nature.com/articles/nature14539), Nature (2015) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Neural Networks\n",
    "\n",
    "Last time, we introduced *Neural Networks* and discussed how they can be used for classification and regression.\n",
    "\n",
    "There are many different *network architectures* for Neural Networks, but our focus is on **Multi-layer Perceptrons**. Here, there is an *input layer*, typically drawn on the left hand side and an *output layer*, typically drawn on the right hand side. The middle layers are called *hidden layers*. \n",
    "\n",
    "\n",
    "<img src=\"Colored_neural_network.svg\" title=\"https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg\" \n",
    "width=\"300\">\n",
    "\n",
    "Given a set of features $X = x^0 = \\{x_1, x_2, ..., x_n\\}$ and a target $y$, a neural network works as follows. \n",
    "\n",
    "\n",
    "Each layer applies an affine transformation and an [activation function](https://en.wikipedia.org/wiki/Activation_function) (e.g., ReLU, hyperbolic tangent, or logistic) to the output of the previous layer: \n",
    "$$\n",
    "x^{j} = f ( A^{j} x^{j-1} + b^j ). \n",
    "$$\n",
    "At the $j$-th hidden layer, the input is represented as the composition of $j$ such mappings. An additional function, *e.g.* [softmax](https://en.wikipedia.org/wiki/Softmax_function), is applied to the output layer to give the prediction, $\\hat y$, for classification or regression. \n",
    "\n",
    "<img src=\"activationFct.png\" \n",
    "title=\"see Géron, Ch. 10\" \n",
    "width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Softmax function for classificaton \n",
    "\n",
    "The *softmax function*, $\\sigma:\\mathbb{R}^K \\to (0,1)^K$ is defined by\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\n",
    "\\qquad \\qquad \\textrm{for } j=1, \\ldots, K.\n",
    "$$\n",
    "Note that each component is in the range $(0,1)$ and the values sum to 1. We interpret $\\sigma(\\mathbf{z})_j$ as the probability that $\\mathbf{z}$ is a member of class $j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Neural networks uses a loss function of the form \n",
    "$$\n",
    "Loss(\\hat{y},y,W) =  \\frac{1}{2} \\sum_{i=1}^n g(\\hat{y}_i(W),y_i) + \\frac{\\alpha}{2} \\|W\\|_2^2\n",
    "$$\n",
    "Here, \n",
    "+ $y_i$ is the label for the $i$-th example, \n",
    "+ $\\hat{y}_i(W)$ is the predicted label for the $i$-th example, \n",
    "+ $g$ is a function that measures the error, typically $L^2$ difference for regression or cross-entropy for classification, and \n",
    "+ $\\alpha$ is a regularization parameter. \n",
    "\n",
    "Starting from initial random weights, the loss function is minimized by repeatedly updating these weights. Various **optimization methods** can be used, *e.g.*, \n",
    "+ gradient descent method \n",
    "+ quasi-Newton method,\n",
    "+ stochastic gradient descent, or \n",
    "+ ADAM. \n",
    "\n",
    "There are various parameters associated with each method that must be tuned. \n",
    "\n",
    "**Back propagation** is a way of using the chain rule from calculus to compute the gradient of the $Loss$ function for optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks in scikit-learn\n",
    "\n",
    "In the previous lecture, we used Neural Network implementations in scikit-learn to do both classification and regression:\n",
    "+ [multi-layer perceptron (MLP) classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "+ [multi-layer perceptron (MLP) regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "\n",
    "However, there are several limitations to the scikit-learn implementation: \n",
    "- no GPU support\n",
    "- limited network architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks with TensorFlow\n",
    "\n",
    "Today, we'll use [TensorFlow](https://github.com/tensorflow/tensorflow) to train a Neural Network. \n",
    "\n",
    "TensorFlow is an open-source library designed for large-scale machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Installing TensorFlow\n",
    "\n",
    "Instructions for installing TensorFlow are available at [the tensorflow install page](https://www.tensorflow.org/install).\n",
    "\n",
    "It is recommended that you use the command: \n",
    "```\n",
    "pip install tensorflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "TensorFlow represents computations by connecting op (operation) nodes into a computation graph.\n",
    "\n",
    "<img src=\"graph.png\" \n",
    "title=\"An example of computational graph\" \n",
    "width=\"400\">\n",
    "\n",
    "A TensorFlow program usually has two components:\n",
    "+ In the *construction phase*, a computational graph is built. During this phase, no computations are performed and the variables are not yet initialized. \n",
    "+ In the *execution phase*, the graph is evaluated, typically many times. In this phase, the each operation is given to a CPU or GPU, variables are initialized, and functions can be evaluted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# construction phase\n",
    "x = tf.Variable(3)\n",
    "y = tf.Variable(4)\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "# execution phase\n",
    "with tf.Session() as sess: # initializes a \"session\" \n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    print(f.eval())\n",
    "\n",
    "\n",
    "# alternatively all variables cab be initialized as follows\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess: # initializes a \"session\" \n",
    "    init.run() # initializes all the variables\n",
    "    print(f.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autodiff\n",
    "\n",
    "TensorFlow can automatically compute the derivative of functions using [```gradients```](https://www.tensorflow.org/api_docs/python/tf/gradients). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 16.0]\n"
     ]
    }
   ],
   "source": [
    "# construction phase\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0)\n",
    "f = x + 2*y*y + 2\n",
    "grads = tf.gradients(f,[x,y])\n",
    "\n",
    "# execution phase\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # initializes all variables\n",
    "    print([g.eval() for g in grads])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is enormously helpful since training a NN requires the derivate of the loss function with respect to the parameters (and there are a lot of parameters). This is computed using backpropagation (chain rule) and TensorFlow does this work for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Exercise:** Use TensorFlow to compute the derivative of $f(x) = e^x$ at $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization methods\n",
    "Tensorflow also has several built-in optimization methods.\n",
    "\n",
    "Other optimization methods in TensorFlow:\n",
    "+ [```tf.train.Optimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer)\n",
    "+ [```tf.train.GradientDescentOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer)\n",
    "+ [```tf.train.AdadeltaOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdadeltaOptimizer)\n",
    "+ [```tf.train.AdagradOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdagradOptimizer)\n",
    "+ [```tf.train.AdagradDAOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdagradDAOptimizer)\n",
    "+ [```tf.train.MomentumOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer)\n",
    "+ [```tf.train.AdamOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer)\n",
    "+ [```tf.train.FtrlOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/FtrlOptimizer)\n",
    "+ [```tf.train.ProximalGradientDescentOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalGradientDescentOptimizer)\n",
    "+ [```tf.train.ProximalAdagradOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer)\n",
    "+ [```tf.train.RMSPropOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/RMSPropOptimizer)\n",
    "\n",
    "For more information, see the [TensorFlow training webpage](https://www.tensorflow.org/api_guides/python/train). \n",
    "\n",
    "\n",
    "Let's see how to use the [```GradientDescentOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.0, 409.0]\n",
      "[1.0980968, 0.0, 1.2058167]\n",
      "[0.40193906, 0.0, 0.161555]\n",
      "[0.14712274, 0.0, 0.0216451]\n",
      "[0.053851694, 0.0, 0.0029000049]\n",
      "[0.019711465, 0.0, 0.00038854184]\n",
      "[0.0072150305, 0.0, 5.2056665e-05]\n",
      "[0.0026409342, 0.0, 6.9745333e-06]\n",
      "[0.00096666755, 0.0, 9.344461e-07]\n",
      "[0.00035383157, 0.0, 1.2519678e-07]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0, trainable=True)\n",
    "y = tf.Variable(2.0, trainable=True)\n",
    "f = x*x + 100*y*y\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=5e-3).minimize(f)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        if i%100 == 0: print(sess.run([x,y,f]))\n",
    "        sess.run(opt)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using another optimizer, such as the [```MomentumOptimizer```](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer), \n",
    "has similiar syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.0, 409.0]\n",
      "[0.043930665, 2.0290405e-15, 0.0019299033]\n",
      "[0.0006126566, -1.547466e-30, 3.753481e-07]\n",
      "[8.544106e-06, 0.0, 7.300175e-11]\n",
      "[1.1915596e-07, 0.0, 1.4198143e-14]\n",
      "[1.6617479e-09, 0.0, 2.761406e-18]\n",
      "[2.3174716e-11, 0.0, 5.3706746e-22]\n",
      "[3.2319424e-13, 0.0, 1.0445451e-25]\n",
      "[4.5072626e-15, 0.0, 2.0315416e-29]\n",
      "[6.285822e-17, 0.0, 3.951156e-33]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0, trainable=True)\n",
    "y = tf.Variable(2.0, trainable=True)\n",
    "f = x*x + 100*y*y\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=1e-2,momentum=.5).minimize(f)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        if i%100 == 0: print(sess.run([x,y,f]))\n",
    "        sess.run(opt)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use TensorFlow to find the minimum of the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function): \n",
    "$$\n",
    "f(x,y) = (x-1)^2 + 100*(y-x^2)^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifying the MNIST handwritten digit dataset\n",
    "\n",
    "We now use TensorFlow to classify the handwritten digits in the MNIST dataset. \n",
    "\n",
    "### Using plain TensorFlow\n",
    "We'll first follow [Géron, Ch. 10](https://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb) to build a NN using plain TensorFlow. \n",
    "\n",
    "\n",
    "\n",
    "#### Construction phase\n",
    "\n",
    "+ We specify the number of inputs and outputs and the size of each layer. Here the images are 28x28 and there are 10 classes (each corresponding to a digit). We'll choose 2 hidden layers, with 300 and 100 neurons respectively. \n",
    "\n",
    "+ Placeholder nodes are used to represent the training data and targets. We use the ```None``` keyword to leave the shape (of the training batch) unspecified. \n",
    "\n",
    "+ We add layers to the NN using the ```layers.dense()``` function. In each case, we specify the input, and the size of the layer. We also specify the activation function used in each layer. Here, we choose the ReLU function. \n",
    "\n",
    "+ We specify that the output of the NN will be a softmax function. The loss function is cross entropy. \n",
    "\n",
    "+ We then specify that we'll use the [GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) \n",
    "with a learning rate of 0.01. \n",
    "\n",
    "+ Finally, we specify how the model will be evaluated. The [```in_top_k```](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) function checks to see if the  targets are in the top k predictions. \n",
    "\n",
    "We then initialize all of the variables and create an object to save the model using the [```saver()```](https://www.tensorflow.org/programmers_guide/saved_model) function. \n",
    "\n",
    "#### Execution phase\n",
    "\n",
    "At each *epoch*, the code breaks the training batch into mini-batches of size 50. Cycling through the mini-batches, it uses gradient descent to train the NN. The accuracy for both the training and test datasets are evaluated.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper code\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-8eb9dd318214>:14: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# construction phase\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    #y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.9 Validation accuracy: 0.9055\n",
      "1 Batch accuracy: 0.9 Validation accuracy: 0.9208\n",
      "2 Batch accuracy: 0.94 Validation accuracy: 0.9331\n",
      "3 Batch accuracy: 0.94 Validation accuracy: 0.9406\n",
      "4 Batch accuracy: 1.0 Validation accuracy: 0.9444\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9481\n",
      "6 Batch accuracy: 0.98 Validation accuracy: 0.9537\n",
      "7 Batch accuracy: 0.98 Validation accuracy: 0.9566\n",
      "8 Batch accuracy: 0.98 Validation accuracy: 0.9591\n",
      "9 Batch accuracy: 1.0 Validation accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "# execution phase\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "#n_batches = 50\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Since the NN has been saved, we can use it for classification using the [```saver.restore```](https://www.tensorflow.org/programmers_guide/saved_model) function. \n",
    "\n",
    "We can also print the confusion matrix using [```confusion_matrix```](https://www.tensorflow.org/api_docs/python/tf/confusion_matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "[[ 969    0    1    1    0    3    1    2    2    1]\n",
      " [   0 1115    2    2    0    1    4    2    9    0]\n",
      " [   7    1  980   12    5    0    6    9   11    1]\n",
      " [   1    0    2  984    0    1    0   10    7    5]\n",
      " [   1    0    3    1  930    0    7    1    5   34]\n",
      " [  10    2    1   25    3  820   10    1   14    6]\n",
      " [   8    3    0    2    9    6  925    0    5    0]\n",
      " [   0   10   12    4    2    0    0  984    2   14]\n",
      " [   3    2    3   13    4    1    8    8  929    3]\n",
      " [   5    7    0   13   11    2    1    5    4  961]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_path)\n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorFlow's Keras API \n",
    "\n",
    "Next, we'll use TensorFlow's Keras API to build a NN for the MNIST dataset. \n",
    "\n",
    "[Keras](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. We'll use it with TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 54s 892us/sample - loss: 0.2186 - acc: 0.9348\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 52s 867us/sample - loss: 0.0963 - acc: 0.9698\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 52s 874us/sample - loss: 0.0687 - acc: 0.9786\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 53s 882us/sample - loss: 0.0537 - acc: 0.9823\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 50s 838us/sample - loss: 0.0428 - acc: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4bf07470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(rate=0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# specifiy optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.0719 - acc: 0.9791\n",
      "loss 0.07185092351303901\n",
      "acc 0.9791\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "names = model.metrics_names\n",
    "for ii in np.arange(len(names)):\n",
    "    print(names[ii],score[ii])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 974    1    0    0    0    0    3    1    0    1]\n",
      " [   0 1124    3    2    0    1    2    0    3    0]\n",
      " [   5    0 1018    2    1    0    1    1    3    1]\n",
      " [   0    1    3  997    0    0    0    2    2    5]\n",
      " [   1    0    1    1  971    0    3    0    0    5]\n",
      " [   2    0    0   20    3  852    5    1    6    3]\n",
      " [   2    3    0    1    7    3  940    0    2    0]\n",
      " [   3    6   10    6    1    0    0  984    4   14]\n",
      " [   4    0    7    6    1    3    3    3  945    2]\n",
      " [   2    2    0    4   12    1    0    1    1  986]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a pre-trained network\n",
    "\n",
    "There are many examples of pre-trained NN that can be accessed [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications). \n",
    "These NN are very large, having been trained on giant computers using massive datasets. \n",
    "\n",
    "It can be very useful to initialize a NN using one of these. This is called [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). \n",
    "\n",
    "\n",
    "We'll use a NN that was pretrained for image recognition. This NN was trained on the  [ImageNet](http://www.image-net.org/) project, which contains > 14 million images belonging to > 20,000 classes (synsets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = tf.keras.applications.VGG16(weights='imagenet',include_top=True)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02098105', 'soft-coated_wheaten_terrier', 0.3554158), ('n02105641', 'Old_English_sheepdog', 0.23714595), ('n02095314', 'wire-haired_fox_terrier', 0.13490717), ('n02091635', 'otterhound', 0.0611032), ('n02093991', 'Irish_terrier', 0.052789364)]\n"
     ]
    }
   ],
   "source": [
    "img_path = 'images/scout1.jpeg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = vgg16.preprocess_input(x)\n",
    "\n",
    "preds = vgg_model.predict(x)\n",
    "print('Predicted:', vgg16.decode_predictions(preds, top=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Exercise:** Repeat the above steps for an image of your own.\n",
    "\n",
    "**Exercise:** There are several [other pre-trained networks in Keras](https://github.com/keras-team/keras-applications). Try these! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some NN topics that we didn't discuss\n",
    "+ Recurrent neural networks (RNN) for time series\n",
    "+ How NN can be used for unsupervised learning problems and [Reinforcement learning problems](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "+ Special layers in NN for image processing \n",
    "+ Using Tensorflow on a GPU \n",
    "+ ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPU vs. GPU\n",
    "\n",
    "[CPUs (Central processing units)](https://en.wikipedia.org/wiki/Central_processing_unit) have just a few cores. The number of processes that a CPU can do in parallel is limited. However, each cores is very fast and is good for sequential tasks. \n",
    "\n",
    "[GPUs (Graphics processing units)](https://en.wikipedia.org/wiki/Graphics_processing_unit) have thousands of cores, so can do many processes in parallel. GPU cores are typically slower and are more limited than CPU cores. However, for the right kind of computations (think matrix multiplication), GPUs are very fast. GPUs also have their own memory and caching systems, which further improves the speed of some computations, but also makes GPUs more difficult to program. (You have to use something like [CUDA](https://en.wikipedia.org/wiki/CUDA)).  \n",
    "\n",
    "TensorFlow can use GPUs to significantly speed up the training NN. See the programmer's guide [here](https://www.tensorflow.org/programmers_guide/using_gpu). "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
